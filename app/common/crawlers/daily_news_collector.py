import logging
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger

from app.common.utils.deepsearch_client import DeepSearchClient
from app.common.rag.rag_service import RagService  # ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” í´ë˜ìŠ¤ëª…ìœ¼ë¡œ ìˆ˜ì •

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)


class DailyNewsCollector:
    """DeepSearch APIë¥¼ ì‚¬ìš©í•œ ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° RAG ì‹œìŠ¤í…œì— ì €ì¥ ì„œë¹„ìŠ¤
    RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì— ì €ì¥í•˜ëŠ” ì„œë¹„ìŠ¤

    ì´ í´ë˜ìŠ¤ëŠ” ë‹¤ìŒ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
    1. ì •ê¸°ì ì¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ë§ (ë§¤ì¼ ìƒˆë²½ 3ì‹œ)
    2. ì €ì¥ëœ í…Œë§ˆ í‚¤ì›Œë“œì— ëŒ€í•œ ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰
    3. ìˆ˜ì§‘ëœ ë‰´ìŠ¤ì˜ RAG ì‹œìŠ¤í…œ ì €ì¥ (ë²¡í„° DBì— ì„ë² ë”©ê³¼ í•¨ê»˜ ì €ì¥)
    4. ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ (ì„œë²„ ì‹œì‘ ì‹œ)

    """

    # ê´€ì‹¬ í…Œë§ˆ ëª©ë¡ - ê° í‚¤ì›Œë“œëŠ” DeepSearch API ì˜ ê²€ìƒ‰ì–´ë¡œ ì‚¬ìš©ë¨
    # ì´ í‚¤ì›Œë“œë“¤ì€ í¬íŠ¸í´ë¦¬ì˜¤ ì¶”ì²œ ì‹œ ì‚¬ìš©ëœëŠ” í…Œë§ˆì™€ ì¼ì¹˜í•˜ë„ë¡ ì„¤ì •ë˜ì–´ ìˆìŒ
    DEFAULT_THEMES = [
        "ì—ë„ˆì§€",
        "ì² ê°•",
        "ê±´ì„¤",
        "ì—¬í–‰",
        "ì€í–‰",
        "ì¦ê¶Œ",
        "ë°˜ë„ì²´",
        "AI",
        "5G",
        "ë¶€ë™ì‚°"
    ]

    # ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ - ì „ì²´ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ í•˜ë‚˜ì˜ ì¸ìŠ¤í„´ìŠ¤ë§Œ ì‚¬ìš©
    _instance = None

    @classmethod
    def get_instance(cls):
        """DailyNewsCollectorì˜ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

        Returns: DailyNewsCollector : ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤

        ì‚¬ìš© ì˜ˆ : collector = DailyNewsCollector.get_instance()
        """
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def __init__(self):
        """RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ë° ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •"""
        # RAG ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        self.rag_service = RagService()

        # ìŠ¤ì¼€ì¤„ëŸ¬ ì´ˆê¸°í™”
        self.scheduler = BackgroundScheduler()
        self._setup_scheduler()

        logger.info("ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")

    def _setup_scheduler(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •"""
        # ë§¤ì¼ ìƒˆë²½ 3ì‹œì— ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤í–‰
        self.scheduler.add_job(
            self.collect_and_store_daily_news,
            CronTrigger(hour=3, minute=0),
            id='daily_news_collection'
        )
        self.scheduler.start()
        logger.info("ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ë¨")

    async def collect_and_store_daily_news(self, themes: List[str] = None, days_back: int = 1) -> int:
        """
        ì§€ì •ëœ í…Œë§ˆì— ëŒ€í•œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  RAG ì‹œìŠ¤í…œì— ì €ì¥

        Args:
            themes: ìˆ˜ì§‘í•  í…Œë§ˆ ëª©ë¡ (ê¸°ë³¸ê°’: None, ê¸°ë³¸ í…Œë§ˆ ì‚¬ìš©)
            days_back: ëª‡ ì¼ ì „ ë°ì´í„°ê¹Œì§€ ìˆ˜ì§‘í• ì§€ (ê¸°ë³¸ê°’: 1)

        Returns:
            ì´ ì €ì¥ëœ ê¸°ì‚¬ ìˆ˜
        """
        if themes is None:
            themes = self.DEFAULT_THEMES

        # ë‚ ì§œ ë²”ìœ„ ì„¤ì •
        today = datetime.now().strftime("%Y-%m-%d")
        past_date = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%d")

        logger.info(f"ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘ - ë‚ ì§œ: {past_date} ~ {today}, í…Œë§ˆ: {', '.join(themes)}")

        total_stored = 0
        theme_stats = {}   # í…Œë§ˆë³„ í†µê³„

        # ê° í…Œë§ˆë³„ë¡œ ì²˜ë¦¬
        for theme in themes:
            logger.info(f"ğŸ”í…Œë§ˆ '{theme}' ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")

            # DeepSearch API í˜¸ì¶œ
            articles = DeepSearchClient.fetch_articles(
                keyword=theme,
                date_from=past_date,
                date_to=today,
                page_limit=10  # í…Œë§ˆë‹¹ ìµœëŒ€ 10í˜ì´ì§€
            )

            if not articles:
                logger.warning(f"í…Œë§ˆ '{theme}'ì— ëŒ€í•œ ìµœì‹  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                theme_stats[theme] = 0
                continue

            logger.info(f"í…Œë§ˆ '{theme}'ì— ëŒ€í•´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ë¨")

            # ìœ íš¨í•œ ê¸°ì‚¬ë§Œ í•„í„°ë§
            valid_articles = []
            for i, article in enumerate(articles):
                try:
                    # None ê°’ ì²´í¬ ë° ì•ˆì „í•œ ì²˜ë¦¬
                    article_id = article.get('id')
                    title = article.get('title_ko', '') or article.get('title', '') or ''
                    summary = article.get('summary_ko', '') or article.get('summary', '') or ''
                    publisher = article.get('publisher', '') or ''
                    published_at = article.get('published_at', '')
                    published_date = published_at.split('T')[0] if published_at else ''

                    # ì¤‘ìš” í•„ë“œ ëˆ„ë½ ì²´í¬
                    if not article_id or not (title or summary):
                        logger.warning(f"âš ï¸ í…Œë§ˆ '{theme}' - ê¸°ì‚¬ #{i + 1}: ì¤‘ìš” í•„ë“œ ëˆ„ë½ìœ¼ë¡œ ê±´ë„ˆëœ€ (ID: {article_id})")
                        continue

                    # ë‚ ì§œ ìœ íš¨ì„± ê²€ì¦
                    if published_date and (published_date < past_date or published_date > today):
                        logger.warning(f"âš ï¸ í…Œë§ˆ '{theme}' - ê¸°ì‚¬ #{i + 1}: ë‚ ì§œ ë²”ìœ„ ë°–ì˜ ê¸°ì‚¬ ({published_date})")

                    # í…Œë§ˆ ì •ë³´ ì¶”ê°€
                    article['theme'] = theme

                    # ê²€ì¦ëœ ê¸°ì‚¬ ì¶”ê°€
                    valid_articles.append(article)

                except Exception as e:
                    logger.warning(f"í…Œë§ˆ '{theme}' - ê¸°ì‚¬ #{i + 1}: ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    continue

            if not valid_articles:
                logger.warning(f"í…Œë§ˆ '{theme}': ìœ íš¨í•œ ê¸°ì‚¬ê°€ ì—†ì–´ ì €ì¥ ê±´ë„ˆëœ€")
                theme_stats[theme] = 0
                continue

            logger.info(f"í…Œë§ˆ '{theme}': ìœ íš¨í•œ ê¸°ì‚¬ {len(valid_articles)}ê°œ ì¤‘ {len(articles)}ê°œ í•„í„°ë§ë¨")

            theme_stored = 0
            # RAG ì„œë¹„ìŠ¤ì˜ save_news_data ë©”ì„œë“œ ì‚¬ìš©í•˜ì—¬ ì €ì¥
            try:
                # ë°°ì¹˜ í¬ê¸° ì§€ì • (ë©”ëª¨ë¦¬ ì´ˆê³¼ ë°©ì§€)
                batch_size = 50
                for i in range(0, len(valid_articles), batch_size):
                    batch = valid_articles[i:i + batch_size]
                    logger.info(
                        f"í…Œë§ˆ '{theme}' - ë°°ì¹˜ {i // batch_size + 1}/{(len(valid_articles) + batch_size - 1) // batch_size}: {len(batch)}ê°œ ê¸°ì‚¬ ì €ì¥ ì‹œì‘")

                    # ë¹„ë™ê¸° ë©”ì„œë“œ ì‹¤í–‰ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ìƒì„±
                    # loop = asyncio.new_event_loop()
                    # asyncio.set_event_loop(loop)
                    #
                    # # save_news_data ë©”ì„œë“œ í˜¸ì¶œ
                    # loop.run_until_complete(self.rag_service.save_news_data(batch))
                    # loop.close()
                    # ì €ì¥ ì‹œì‘ ì‹œê°„
                    start_time = datetime.now()

                    await self.rag_service.save_news_data(batch)

                    logger.info(f"âœ… í…Œë§ˆ '{theme}' - ë°°ì¹˜ {i // batch_size + 1}: {len(batch)}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")
                    total_stored += len(batch)

                    # ì €ì¥ ì†Œìš” ì‹œê°„ ê³„ì‚°
                    elapsed_time = (datetime.now() - start_time).total_seconds()

                    logger.info(
                        f"âœ… í…Œë§ˆ '{theme}' - ë°°ì¹˜ {i // batch_size + 1}: {len(batch)}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
                    theme_stored += len(batch)
                    total_stored += len(batch)

                # í…Œë§ˆë³„ í†µê³„ ì €ì¥
                theme_stats[theme] = total_stored

            except Exception as e:
                logger.error(f"RAG ì‹œìŠ¤í…œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                # ì˜¤ë¥˜ ë””ë²„ê¹…ì„ ìœ„í•œ ì¶”ê°€ ì •ë³´
                import traceback
                logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

            # ì „ì²´ í†µê³„ ë¡œê¹…
        logger.info(f"ğŸ“Š ì¼ì¼ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥ ì™„ë£Œ - ì´ {total_stored}ê°œ ê¸°ì‚¬")
        logger.info("ğŸ“ˆ í…Œë§ˆë³„ ì €ì¥ í†µê³„:")
        for theme, count in theme_stats.items():
            logger.info(f"  - {theme}: {count}ê°œ ê¸°ì‚¬")

        # ì˜¤ë˜ëœ ê¸°ì‚¬ ìë™ ì‚­ì œ (30ì¼ ì „ ì´ì „)
        try:
            deleted_count = await self.rag_service.delete_old_documents(days=30)
            logger.info(f"ğŸ§¹ ì˜¤ë˜ëœ ê¸°ì‚¬ {deleted_count}ê°œ ì‚­ì œ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë˜ëœ ê¸°ì‚¬ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        return total_stored

    async def collect_initial_data(self):
        """ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ (ì•± ì‹œì‘ ì‹œ ì‹¤í–‰)"""
        logger.info("ì´ˆê¸° ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

        try:
            # 30ì¼ì¹˜ ë°ì´í„° ìˆ˜ì§‘
            await self.collect_and_store_daily_news(days_back=30)

            # ë°ì´í„° ìˆ˜ì§‘ í™•ì¸
            await self.check_collected_data()

            logger.info("ğŸ‰ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ë° í™•ì¸ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ì´ˆê¸° ë°ì´í„° ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    def shutdown(self):
        """ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ"""
        if hasattr(self, 'scheduler'):
            self.scheduler.shutdown()
            logger.info("ë‰´ìŠ¤ ìˆ˜ì§‘ ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œë¨")

    async def check_collected_data(self):
        """ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ë°ì´í„° ìƒíƒœ ë¡œê¹…"""
        logger.info("ğŸ” ë²¡í„° DB ë°ì´í„° ìˆ˜ì§‘ í˜„í™© í™•ì¸ ì¤‘...")

        try:
            # ë²¡í„° DB ì—°ê²° í™•ì¸
            collection = self.rag_service.collection

            # ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            all_results = collection.get(include=["metadatas"])

            if not all_results or "ids" not in all_results or not all_results["ids"]:
                logger.warning("âš ï¸ ë²¡í„° DBì— ì €ì¥ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            total_docs = len(all_results["ids"])
            logger.info(f"ğŸ“Š ë²¡í„° DBì— ì´ {total_docs}ê°œ ê¸°ì‚¬ê°€ ì €ì¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

            # ë‚ ì§œë³„ í†µê³„ ìˆ˜ì§‘
            date_counts = {}
            theme_counts = {}

            for metadata in all_results.get("metadatas", []):
                if not metadata:
                    continue

                # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ ë° ì €ì¥
                published_at = metadata.get("published_at", "")
                pub_date = published_at.split("T")[0] if published_at and "T" in published_at else published_at

                if pub_date:
                    date_counts[pub_date] = date_counts.get(pub_date, 0) + 1

                # í…Œë§ˆ ì •ë³´ ì¶”ì¶œ ë° ì €ì¥
                theme = metadata.get("theme", "unknown")
                theme_counts[theme] = theme_counts.get(theme, 0) + 1

            # ë‚ ì§œ ë²”ìœ„ í™•ì¸
            if date_counts:
                dates = sorted(date_counts.keys())
                earliest_date = dates[0] if dates else "ì—†ìŒ"
                latest_date = dates[-1] if dates else "ì—†ìŒ"

                logger.info(f"ğŸ“… ìˆ˜ì§‘ëœ ê¸°ì‚¬ ë‚ ì§œ ë²”ìœ„: {earliest_date} ~ {latest_date}")

                # ë‚ ì§œë³„ ë¶„í¬ ë¡œê¹… (5ì¼ ë‹¨ìœ„ë¡œ ìš”ì•½)
                date_groups = {}
                for date, count in date_counts.items():
                    # ë‚ ì§œë¥¼ 5ì¼ ë‹¨ìœ„ë¡œ ê·¸ë£¹í™”
                    if not date:
                        continue
                    try:
                        date_obj = datetime.strptime(date, "%Y-%m-%d")
                        group_key = date_obj.strftime("%Y-%m-%d") + " ~ " + (date_obj + timedelta(days=4)).strftime(
                            "%Y-%m-%d")
                        date_groups[group_key] = date_groups.get(group_key, 0) + count
                    except ValueError:
                        # ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜ ì²˜ë¦¬
                        continue

                logger.info("ğŸ“† 5ì¼ ë‹¨ìœ„ ê¸°ì‚¬ ë¶„í¬:")
                for group, count in sorted(date_groups.items()):
                    logger.info(f"  - {group}: {count}ê°œ ê¸°ì‚¬")

            # í…Œë§ˆë³„ ë¶„í¬ ë¡œê¹…
            if theme_counts:
                logger.info("ğŸ·ï¸ í…Œë§ˆë³„ ê¸°ì‚¬ ìˆ˜:")
                for theme, count in sorted(theme_counts.items(), key=lambda x: x[1], reverse=True):
                    logger.info(f"  - {theme}: {count}ê°œ ê¸°ì‚¬")

                # ëˆ„ë½ëœ í…Œë§ˆ í™•ì¸
                missing_themes = [t for t in self.DEFAULT_THEMES if t not in theme_counts]
                if missing_themes:
                    logger.warning(f"âš ï¸ ë‹¤ìŒ í…Œë§ˆì˜ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤: {', '.join(missing_themes)}")

            logger.info("âœ… ë°ì´í„° ìˆ˜ì§‘ í˜„í™© í™•ì¸ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")