import re

from datetime import datetime, timedelta

from sentence_transformers import SentenceTransformer
from concurrent.futures import ThreadPoolExecutor
import asyncio
import logging

from app.common.db.vector.vector_util import VectorUtil

embedding_model = SentenceTransformer("BM-K/KoSimCSE-roberta")
vector_util = VectorUtil()
collection = vector_util.get_collection()

comma = ","

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ì“°ë ˆë“œ í’€
executor = ThreadPoolExecutor(max_workers=4)

def clean_text(text: str) -> str:
    """GPT í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ ì •ì œ"""
    text = re.sub(r"<[^>]+>", "", text)  # HTML íƒœê·¸ ì œê±°
    text = re.sub(r"\s+", " ", text)     # ì—°ì† ê³µë°± ì œê±°
    text = text.strip()
    return text

# rag ê¸°ë°˜ ì„œë¹„ìŠ¤ë¥¼ ì œê³µ
class RagService:
    
    def __init__(self):
        """RagService ì´ˆê¸°í™” - ì „ì—­ collection ê°ì²´ ì‚¬ìš©"""
        global collection
        self.collection = collection

    # # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°˜í™˜
    # def get_news_data(self, categories, n_results=5, min_relevance_score=0.3):
    #     """ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤ í•„í„°ë§"""
    #
    #     # ë„ë©”ì¸ ê°•í™” ì¿¼ë¦¬ ìƒì„±
    #     enriched_queries = []
    #     for category in categories:
    #         if "ë°˜ë„ì²´" in category:
    #             enriched_queries.append(f"ë°˜ë„ì²´ ì‚°ì—… ê´€ì ì—ì„œ {category}")
    #         elif "ê¸°ìˆ " in category or "AI" in category:
    #             enriched_queries.append(f"ê¸°ìˆ  ì‚°ì—… ê´€ì ì—ì„œ {category}")
    #         else:
    #             enriched_queries.append(category)
    #
    #     # 1. ì¿¼ë¦¬ë¥¼ ìì—°ì–´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜
    #     query = f"{', '.join(enriched_queries)} ê´€ë ¨ ì‚°ì—… ë™í–¥ ë° ë‰´ìŠ¤ ê¸°ì‚¬"
    #
    #     # ë¡œê¹… ì¶”ê°€
    #     logger.info(f"[RAG] ê°•í™”ëœ ì¿¼ë¦¬: '{query}'")
    #
    #     # 2. ì¿¼ë¦¬ ì„ë² ë”©
    #     query_embedding = embedding_model.encode(query)
    #
    #     # ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§
    #     results = self.collection.query(
    #         query_embeddings=[query_embedding],
    #         n_results=n_results * 3,  # ë” ë§ì€ í›„ë³´ ê²€ìƒ‰
    #         include=["documents", "metadatas", "distances"]  # ê±°ë¦¬ ì •ë³´ í¬í•¨
    #     )
    #
    #     # ê²°ê³¼ì™€ ê´€ë ¨ì„± ì ìˆ˜ ê²°í•©
    #     news_with_scores = []
    #     for doc, meta, distance in zip(
    #             results["documents"][0],
    #             results["metadatas"][0],
    #             results["distances"][0]):
    #
    #         # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)
    #         similarity_score = 1 - min(distance, 1.0)
    #
    #         # 3. í•„í„°ë§ ê¸°ì¤€ì„ 0.1 ë˜ëŠ” 0.2ë¡œ ë‚®ì¶°ë³´ê¸°
    #         if similarity_score >= min_relevance_score:
    #             news_with_scores.append({
    #                 "title": meta.get("title", ""),
    #                 "source": meta.get("publisher", ""),
    #                 "published_date": meta.get("published_at", ""),
    #                 "summary": doc,
    #                 "relevance_score": similarity_score
    #             })
    #
    #     # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ n_results ë°˜í™˜
    #     news_articles = sorted(news_with_scores, key=lambda x: x["relevance_score"], reverse=True)[:n_results]
    #
    #     # 4. ìœ ì‚¬ë„ ë””ë²„ê¹…ìš© ë¡œê·¸
    #     if not news_articles:
    #         logger.warning(f"[RAG] '{categories}' ê´€ë ¨ ë‰´ìŠ¤ê°€ ìœ ì‚¬ë„ ê¸°ì¤€({min_relevance_score}) ë¯¸ë‹¬ë¡œ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
    #     else:
    #         for i, news in enumerate(news_articles):
    #             logger.info(f"[RAG] ì„ íƒëœ ë‰´ìŠ¤ {i+1}: ìœ ì‚¬ë„={news['relevance_score']:.3f} | ì œëª©={news['title'][:40]}")
    #
    #     # í›„ë³´ ì¤‘ì—ì„œ ë¬´ì‘ìœ„ ì¶”ì¶œ
    #     import random
    #     sampled = random.sample(news_with_scores, k=min(n_results, len(news_with_scores)))
    #
    #     return sampled

    def get_news_data(self, categories, n_results=5, min_relevance_score=0.3,
                      add_diversity=False, days_ago=None, filter_by_theme=False):
        """ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤ í•„í„°ë§

        Args:
            categories: ê²€ìƒ‰í•  ì¹´í…Œê³ ë¦¬ ëª©ë¡
            n_results: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            min_relevance_score: ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜
            add_diversity: ë‹¤ì–‘í•œ ì¿¼ë¦¬ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: False - ê¸°ì¡´ ë™ì‘ ìœ ì§€)
            days_ago: ìµœì‹  ê¸°ì‚¬ í•„í„°ë§ ì¼ìˆ˜ (Noneì´ë©´ í•„í„°ë§ ì—†ìŒ)
            filter_by_theme: í…Œë§ˆ ê¸°ë°˜ ì¶”ê°€ í•„í„°ë§ (ê¸°ë³¸ê°’: False - ê¸°ì¡´ ë™ì‘ ìœ ì§€)
        """

        # ê²°ê³¼ ì €ì¥ìš©
        all_results = []
        seen_ids = set()  # ì¤‘ë³µ ë°©ì§€

        # ì¿¼ë¦¬ ëª©ë¡ ì´ˆê¸°í™”
        query_variations = []

        # 1. ê¸°ì¡´ ë„ë©”ì¸ ê°•í™” ì¿¼ë¦¬ ìƒì„± (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
        enriched_queries = []
        for category in categories:
            if "ë°˜ë„ì²´" in category:
                enriched_queries.append(f"ë°˜ë„ì²´ ì‚°ì—… ê´€ì ì—ì„œ {category}")
            elif "ê¸°ìˆ " in category or "AI" in category:
                enriched_queries.append(f"ê¸°ìˆ  ì‚°ì—… ê´€ì ì—ì„œ {category}")
            else:
                enriched_queries.append(category)

        # ê¸°ë³¸ ì¿¼ë¦¬ (ê¸°ì¡´ ë°©ì‹)
        base_query = f"{', '.join(enriched_queries)} ê´€ë ¨ ì‚°ì—… ë™í–¥ ë° ë‰´ìŠ¤ ê¸°ì‚¬"
        query_variations.append(base_query)

        # ë‹¤ì–‘ì„± ì¶”ê°€ ì˜µì…˜ì´ ì¼œì ¸ ìˆìœ¼ë©´ ì¶”ê°€ ì¿¼ë¦¬ ìƒì„±
        if add_diversity:
            simple_query = f"{' '.join(categories)} ìµœì‹  ë™í–¥"
            query_variations.append(simple_query)

        # ê° ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ìˆ˜í–‰
        for i, query in enumerate(query_variations):
            # ë¡œê¹… (ì²« ë²ˆì§¸ëŠ” ê¸°ì¡´ ë¡œê·¸ ë©”ì‹œì§€ ìœ ì§€)
            if i == 0:
                logger.info(f"[RAG] ê°•í™”ëœ ì¿¼ë¦¬: '{query}'")
            else:
                logger.info(f"[RAG] ì¶”ê°€ ì¿¼ë¦¬ #{i}: '{query}'")

            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = embedding_model.encode(query)

            # ê²€ìƒ‰ ì‹¤í–‰
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results * 2,  # ë” ë§ì€ í›„ë³´ ê²€ìƒ‰
                include=["documents", "metadatas", "distances"]
            )

            # ê²°ê³¼ ì²˜ë¦¬
            for doc, meta, distance in zip(
                    results["documents"][0],
                    results["metadatas"][0],
                    results["distances"][0]):

                # ê¸°ë³¸ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (ê¸°ì¡´ ë°©ì‹)
                similarity_score = 1 - min(distance, 1.0)

                # ê¸°ë³¸ ìœ ì‚¬ë„ í•„í„°ë§ (ê¸°ì¡´ ë°©ì‹)
                if similarity_score < min_relevance_score:
                    continue

                # ê³ ìœ  ID ìƒì„± (ë¬¸ì„œ ì¤‘ë³µ ë°©ì§€ìš©)
                doc_id = meta.get("id", "") or hash(doc + meta.get("title", ""))
                if doc_id in seen_ids:
                    continue
                seen_ids.add(doc_id)

                # ì„ íƒì  í…Œë§ˆ í•„í„°ë§
                if filter_by_theme and meta.get("theme"):
                    theme_match = False
                    for category in categories:
                        if category.lower() in meta.get("theme", "").lower():
                            theme_match = True
                            break
                    if not theme_match and i == 0:  # ì²«ë²ˆì§¸(ê¸°ë³¸) ì¿¼ë¦¬ì—ì„œëŠ” í…Œë§ˆ í™•ì¸
                        # í…Œë§ˆ ë§¤ì¹˜ ì—†ìœ¼ë©´ ìŠ¤ì½”ì–´ ì•½ê°„ ê°ì†Œ
                        similarity_score *= 0.95

                # ì„ íƒì  ë‚ ì§œ í•„í„°ë§
                if days_ago and meta.get("published_at"):
                    try:
                        pub_date = meta.get("published_at", "").split("T")[0]
                        cutoff_date = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%d')
                        # ì˜¤ë˜ëœ ê¸°ì‚¬ëŠ” ìŠ¤ì½”ì–´ ì•½ê°„ ê°ì†Œ
                        if pub_date < cutoff_date:
                            similarity_score *= 0.95
                    except:
                        pass  # ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜ ë¬´ì‹œ

                # ê²°ê³¼ ì¶”ê°€
                all_results.append({
                    "title": meta.get("title", ""),
                    "source": meta.get("publisher", ""),
                    "published_date": meta.get("published_at", ""),
                    "summary": doc,
                    "relevance_score": similarity_score
                })

        # ê¸°ì¡´ ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ ê²°ê³¼ ë°˜í™˜
        if not all_results and not query_variations:
            # ê¸°ì¡´ ì½”ë“œì˜ ê¸°ë³¸ ê²€ìƒ‰ (í˜¸í™˜ì„± ìœ ì§€)
            return self._original_search_method(categories, n_results, min_relevance_score)

        # ìµœì¢… ê²°ê³¼ ì •ë ¬ ë° í•„í„°ë§
        news_articles = sorted(all_results, key=lambda x: x["relevance_score"], reverse=True)[:n_results]

        # ë¡œê¹… (ê¸°ì¡´ ë°©ì‹ ìœ ì§€)
        if not news_articles:
            logger.warning(f"[RAG] '{categories}' ê´€ë ¨ ë‰´ìŠ¤ê°€ ìœ ì‚¬ë„ ê¸°ì¤€({min_relevance_score}) ë¯¸ë‹¬ë¡œ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            for i, news in enumerate(news_articles):
                logger.info(f"[RAG] ì„ íƒëœ ë‰´ìŠ¤ {i + 1}: ìœ ì‚¬ë„={news['relevance_score']:.3f} | ì œëª©={news['title'][:40]}")

        return news_articles

    # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì €ì¥
    async def save_news_data(self, news_articles):
        """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë²¡í„° DBì— ì €ì¥"""
        if not news_articles:
            logger.warning("ì €ì¥í•  ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        loop = asyncio.get_event_loop()
        success_count = 0
        error_count = 0

        for article in news_articles:
            try:
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                article_id = str(article.get("id", ""))
                if not article_id:
                    logger.warning("ê¸°ì‚¬ IDê°€ ì—†ì–´ ê±´ë„ˆëœ€")
                    error_count += 1
                    continue

                # ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ë¡œì§ ì¶”ê°€
                existing_ids = self.collection.get(ids=[article_id])["ids"]

                if existing_ids:
                    logger.info(f"ì¤‘ë³µ ê¸°ì‚¬ ìƒëµ : ID = {article_id}")
                    continue

                # í…ìŠ¤íŠ¸ í•„ë“œ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
                summary_ko = article.get("summary_ko", "") or ""
                summary = article.get("summary", "") or ""
                reason = article.get("reason", "") or ""

                summary_text = summary_ko if summary_ko else summary
                full_text = f"{summary_text}\n{reason}".strip()
                full_text = clean_text(full_text)  # âœ… ì •ì œ í•¨ìˆ˜ ì ìš©

                if not full_text:
                    logger.warning(f"ê¸°ì‚¬ {article_id}ì˜ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆì–´ ê±´ë„ˆëœ€")
                    error_count += 1
                    continue

                # ì„ë² ë”© ìƒì„±
                embedding = await loop.run_in_executor(executor, embedding_model.encode, full_text)

                # ë©”íƒ€ë°ì´í„° êµ¬ì„± (None ê°’ ì²˜ë¦¬)
                metadata = {
                    "title": article.get("title_ko", "") or article.get("title", "") or "",
                    "published_at": article.get("published_at", ""),
                    "importance": article.get("importance", "medium"),
                    "publisher": article.get("publisher", ""),
                    "theme": article.get("theme", "")
                }

                # ë©”íƒ€ë°ì´í„°ì˜ None ê°’ ì²˜ë¦¬
                for key, value in metadata.items():
                    if value is None:
                        metadata[key] = ""

                # logger.info(f"ğŸ§ª ê¸°ì‚¬ ì €ì¥ ì‹œë„: ID={article_id}, ì œëª©={metadata['title'][:30]}, ì„ë² ë”© ê¸¸ì´={len(embedding)}")

                # ë²¡í„° DBì— ì €ì¥
                self.collection.add(
                    documents=[full_text],
                    embeddings=[embedding.tolist()],
                    ids=[article_id],
                    metadatas=[metadata]
                )

                success_count += 1

                # logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {article_id}")

            except Exception as e:
                logger.error(f"ê¸°ì‚¬ {article.get('id', 'unknown')} ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                error_count += 1

        logger.info(f"ë‰´ìŠ¤ ë°ì´í„° ì €ì¥ ì™„ë£Œ: ì„±ê³µ {success_count}ê±´, ì‹¤íŒ¨ {error_count}ê±´")
        return success_count

    # ì˜¤ë˜ëœ ê¸°ì‚¬ ë°ì´í„° ì‚­ì œ
    async def delete_old_documents(self, days: int = 30):
        """ì§€ì •ëœ ë‚ ì§œ ì´ì „ì˜ ë¬¸ì„œë¥¼ ì‚­ì œ (ê¸°ë³¸ 30ì¼ ì´ì „)

        :param days:
        :return:
        """
        # í˜„ì¬ ë‚ ì§œì—ì„œ daysì¼ ì´ì „ ê³„ì‚°
        cutoff_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        logger.info(f"ğŸ§¹ {cutoff_date} ì´ì „ì˜ ê¸°ì‚¬ ì œê±° ì‹œë„ ì¤‘...")

        try:
            # 1. ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
            all_results = self.collection.get(include=["metadatas"])

            # 2. íŒŒì´ì¬ì—ì„œ ë‚ ì§œ ë¹„êµ ìˆ˜í–‰
            old_ids = []
            for i, metadata in enumerate(all_results.get("metadatas", [])):
                if not metadata:
                    continue

                published_at = metadata.get("published_at", "")
                if not published_at:
                    continue

                # ë‚ ì§œ í˜•ì‹ ë¹„êµ (YYYY-MM-DD í˜•ì‹ ê°€ì •)
                if published_at < cutoff_date:
                    old_ids.append(all_results["ids"][i])

            if not old_ids:
                logger.info("ğŸŸ¢ ì‚­ì œ ëŒ€ìƒ ë¬¸ì„œ ì—†ìŒ")
                return 0

            logger.info(f"ğŸ—‘ {len(old_ids)}ê°œ ë¬¸ì„œ ì‚­ì œ ì‹œì‘...")

            # 3. ì˜¤ë˜ëœ ë¬¸ì„œ ì‚­ì œ
            self.collection.delete(ids=old_ids)

            logger.info(f"âœ… {len(old_ids)}ê°œ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")
            return len(old_ids)

        except Exception as e:
            logger.error(f"ì˜¤ë˜ëœ ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return 0