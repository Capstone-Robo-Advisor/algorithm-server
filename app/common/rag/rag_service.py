import re

from datetime import datetime, timedelta

from sentence_transformers import SentenceTransformer
from concurrent.futures import ThreadPoolExecutor
import asyncio
import logging

from app.common.db.vector.vector_util import VectorUtil

embedding_model = SentenceTransformer("jhgan/ko-sroberta-multitask")
vector_util = VectorUtil()
collection = vector_util.get_collection()

comma = ","

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

# ì“°ë ˆë“œ í’€
executor = ThreadPoolExecutor(max_workers=4)

def clean_text(text: str) -> str:
    """GPT í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ ì •ì œ"""
    text = re.sub(r"<[^>]+>", "", text)  # HTML íƒœê·¸ ì œê±°
    text = re.sub(r"\s+", " ", text)     # ì—°ì† ê³µë°± ì œê±°
    text = text.strip()
    return text

# rag ê¸°ë°˜ ì„œë¹„ìŠ¤ë¥¼ ì œê³µ
class RagService:
    
    def __init__(self):
        """RagService ì´ˆê¸°í™” - ì „ì—­ collection ê°ì²´ ì‚¬ìš©"""
        global collection
        self.collection = collection

    # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë°˜í™˜
    def get_news_data(self, categories, n_results=5, min_relevance_score=0.6):
        """ê´€ë ¨ì„± ì ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë‰´ìŠ¤ í•„í„°ë§"""
        query = comma.join(categories)
        query_embedding = embedding_model.encode(query)

        # ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=n_results * 2,  # ë” ë§ì€ í›„ë³´ ê²€ìƒ‰
            include=["documents", "metadatas", "distances"]  # ê±°ë¦¬ ì •ë³´ í¬í•¨
        )

        # ê²°ê³¼ì™€ ê´€ë ¨ì„± ì ìˆ˜ ê²°í•©
        news_with_scores = []
        for i, (doc, meta, distance) in enumerate(zip(
                results["documents"][0],
                results["metadatas"][0],
                results["distances"][0])):

            # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜ (1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìœ ì‚¬)
            similarity_score = 1 - min(distance, 1.0)

            if similarity_score >= min_relevance_score:
                news_with_scores.append({
                    "title": meta.get("title", ""),
                    "source": meta.get("publisher", ""),
                    "published_date": meta.get("published_at", ""),
                    "summary": doc,
                    "relevance_score": similarity_score
                })

        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ n_results ë°˜í™˜
        news_articles = sorted(news_with_scores, key=lambda x: x["relevance_score"], reverse=True)[:n_results]
        return news_articles

    # ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì €ì¥
    async def save_news_data(self, news_articles):
        """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ë²¡í„° DBì— ì €ì¥"""
        if not news_articles:
            logger.warning("ì €ì¥í•  ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        loop = asyncio.get_event_loop()
        success_count = 0
        error_count = 0

        for article in news_articles:
            try:
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                article_id = str(article.get("id", ""))
                if not article_id:
                    logger.warning("ê¸°ì‚¬ IDê°€ ì—†ì–´ ê±´ë„ˆëœ€")
                    error_count += 1
                    continue

                # ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ë¡œì§ ì¶”ê°€
                existing_ids = self.collection.get(ids=[article_id])["ids"]

                if existing_ids:
                    logger.info(f"ì¤‘ë³µ ê¸°ì‚¬ ìƒëµ : ID = {article_id}")
                    continue

                # í…ìŠ¤íŠ¸ í•„ë“œ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
                summary_ko = article.get("summary_ko", "") or ""
                summary = article.get("summary", "") or ""
                reason = article.get("reason", "") or ""

                summary_text = summary_ko if summary_ko else summary
                full_text = f"{summary_text}\n{reason}".strip()
                full_text = clean_text(full_text)  # âœ… ì •ì œ í•¨ìˆ˜ ì ìš©

                if not full_text:
                    logger.warning(f"ê¸°ì‚¬ {article_id}ì˜ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ ìˆì–´ ê±´ë„ˆëœ€")
                    error_count += 1
                    continue

                # ì„ë² ë”© ìƒì„±
                embedding = await loop.run_in_executor(executor, embedding_model.encode, full_text)

                # ë©”íƒ€ë°ì´í„° êµ¬ì„± (None ê°’ ì²˜ë¦¬)
                metadata = {
                    "title": article.get("title_ko", "") or article.get("title", "") or "",
                    "published_at": article.get("published_at", ""),
                    "importance": article.get("importance", "medium"),
                    "publisher": article.get("publisher", ""),
                    "theme": article.get("theme", "")
                }

                # ë©”íƒ€ë°ì´í„°ì˜ None ê°’ ì²˜ë¦¬
                for key, value in metadata.items():
                    if value is None:
                        metadata[key] = ""

                # logger.info(f"ğŸ§ª ê¸°ì‚¬ ì €ì¥ ì‹œë„: ID={article_id}, ì œëª©={metadata['title'][:30]}, ì„ë² ë”© ê¸¸ì´={len(embedding)}")

                # ë²¡í„° DBì— ì €ì¥
                self.collection.add(
                    documents=[full_text],
                    embeddings=[embedding.tolist()],
                    ids=[article_id],
                    metadatas=[metadata]
                )

                success_count += 1

                # logger.info(f"âœ… ì €ì¥ ì™„ë£Œ: {article_id}")

            except Exception as e:
                logger.error(f"ê¸°ì‚¬ {article.get('id', 'unknown')} ì €ì¥ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                error_count += 1

        logger.info(f"ë‰´ìŠ¤ ë°ì´í„° ì €ì¥ ì™„ë£Œ: ì„±ê³µ {success_count}ê±´, ì‹¤íŒ¨ {error_count}ê±´")
        return success_count

    # ì˜¤ë˜ëœ ê¸°ì‚¬ ë°ì´í„° ì‚­ì œ
    async def delete_old_documents(self, days: int = 30):
        """ì§€ì •ëœ ë‚ ì§œ ì´ì „ì˜ ë¬¸ì„œë¥¼ ì‚­ì œ (ê¸°ë³¸ 30ì¼ ì´ì „)

        :param days:
        :return:
        """
        # í˜„ì¬ ë‚ ì§œì—ì„œ daysì¼ ì´ì „ ê³„ì‚°
        cutoff_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')
        logger.info(f"ğŸ§¹ {cutoff_date} ì´ì „ì˜ ê¸°ì‚¬ ì œê±° ì‹œë„ ì¤‘...")

        try:
            # 1. ëª¨ë“  ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
            all_results = self.collection.get(include=["metadatas"])

            # 2. íŒŒì´ì¬ì—ì„œ ë‚ ì§œ ë¹„êµ ìˆ˜í–‰
            old_ids = []
            for i, metadata in enumerate(all_results.get("metadatas", [])):
                if not metadata:
                    continue

                published_at = metadata.get("published_at", "")
                if not published_at:
                    continue

                # ë‚ ì§œ í˜•ì‹ ë¹„êµ (YYYY-MM-DD í˜•ì‹ ê°€ì •)
                if published_at < cutoff_date:
                    old_ids.append(all_results["ids"][i])

            if not old_ids:
                logger.info("ğŸŸ¢ ì‚­ì œ ëŒ€ìƒ ë¬¸ì„œ ì—†ìŒ")
                return 0

            logger.info(f"ğŸ—‘ {len(old_ids)}ê°œ ë¬¸ì„œ ì‚­ì œ ì‹œì‘...")

            # 3. ì˜¤ë˜ëœ ë¬¸ì„œ ì‚­ì œ
            self.collection.delete(ids=old_ids)

            logger.info(f"âœ… {len(old_ids)}ê°œ ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ")
            return len(old_ids)

        except Exception as e:
            logger.error(f"ì˜¤ë˜ëœ ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return 0